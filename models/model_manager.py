"""
æ¨¡å‹ç®¡ç†æ¨¡å—
æ”¯æŒ API æ¨¡å‹ï¼ˆDeepSeekï¼‰å’Œæœ¬åœ°æ¨¡å‹ï¼ˆOllamaï¼‰çš„åŠ¨æ€åˆ‡æ¢
æ³¨æ„ï¼šæœ¬åœ°æ¨¡å‹ï¼ˆOllamaï¼‰æš‚æ—¶ç¦ç”¨ï¼Œä¼˜å…ˆä½¿ç”¨ API
"""
import logging
from typing import Optional, Dict, Any, Iterator
from abc import ABC, abstractmethod

try:
    from openai import OpenAI
except ImportError:
    OpenAI = None

from config.config_manager import get_config

logger = logging.getLogger(__name__)


class BaseLLM(ABC):
    """LLM åŸºç±»"""
    
    @abstractmethod
    def chat_completion(
        self,
        messages: list,
        temperature: float = 0.7,
        max_tokens: int = 2000,
        stream: bool = False
    ) -> Any:
        """èŠå¤©è¡¥å…¨"""
        pass
    
    @abstractmethod
    def is_available(self) -> bool:
        """æ£€æŸ¥æ¨¡å‹æ˜¯å¦å¯ç”¨"""
        pass


class DeepSeekLLM(BaseLLM):
    """DeepSeek API æ¨¡å‹"""
    
    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ– DeepSeek API å®¢æˆ·ç«¯
        
        Args:
            config: æ¨¡å‹é…ç½®å­—å…¸
        """
        if OpenAI is None:
            raise ImportError("éœ€è¦å®‰è£… openai åº“: pip install openai")
        
        self.api_key = config.get('api_key')
        if not self.api_key:
            raise ValueError("DeepSeek API key æœªè®¾ç½®")
        
        self.base_url = config.get('base_url', 'https://api.deepseek.com')
        self.model_name = config.get('model_name', 'deepseek-chat')
        self.temperature = config.get('temperature', 0.7)
        self.max_tokens = config.get('max_tokens', 2000)
        
        self.client = OpenAI(
            api_key=self.api_key,
            base_url=self.base_url
        )
        logger.info(f"DeepSeek API å®¢æˆ·ç«¯å·²åˆå§‹åŒ–: {self.base_url}")
    
    def chat_completion(
        self,
        messages: list,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        stream: bool = False
    ):
        """
        è°ƒç”¨ DeepSeek API è¿›è¡ŒèŠå¤©è¡¥å…¨
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§ token æ•°
            stream: æ˜¯å¦æµå¼è¿”å›
            
        Returns:
            API å“åº”å¯¹è±¡æˆ–æµå¼è¿­ä»£å™¨
        """
        try:
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=messages,
                temperature=temperature or self.temperature,
                max_tokens=max_tokens or self.max_tokens,
                stream=stream
            )
            return response
        except Exception as e:
            logger.error(f"DeepSeek API è°ƒç”¨å¤±è´¥: {e}")
            raise
    
    def is_available(self) -> bool:
        """æ£€æŸ¥ DeepSeek API æ˜¯å¦å¯ç”¨"""
        try:
            # ç®€å•çš„å¥åº·æ£€æŸ¥ï¼šå‘é€ä¸€ä¸ªçŸ­æ¶ˆæ¯
            test_response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[{"role": "user", "content": "test"}],
                max_tokens=5
            )
            return test_response is not None
        except Exception:
            return False


# ============================================================================
# Ollama æœ¬åœ°æ¨¡å‹æ”¯æŒå·²å¼ƒç”¨
# ============================================================================
class LocalLLM(BaseLLM):
    """æœ¬åœ°æ¨¡å‹ï¼ˆOllama/vLLMï¼Œå…¼å®¹ OpenAI APIï¼‰- å·²å¼ƒç”¨"""
    
    def __init__(self, config: Dict[str, Any]):
        """å·²å¼ƒç”¨ï¼šOllama æœ¬åœ°æ¨¡å‹æ”¯æŒ"""
        logger.warning("LocalLLM å·²å¼ƒç”¨ï¼Œè¯·ä½¿ç”¨ DeepSeek API")
        pass
    
    def chat_completion(
        self,
        messages: list,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        stream: bool = False
    ):
        """å·²å¼ƒç”¨ï¼šOllama æœ¬åœ°æ¨¡å‹æ”¯æŒ"""
        raise NotImplementedError("LocalLLM å·²å¼ƒç”¨ï¼Œè¯·ä½¿ç”¨ DeepSeek API")
    
    def is_available(self) -> bool:
        """å·²å¼ƒç”¨ï¼šOllama æœ¬åœ°æ¨¡å‹æ”¯æŒ"""
        return False


class ModelManager:
    """æ¨¡å‹ç®¡ç†å™¨ï¼Œæ”¯æŒåŠ¨æ€åˆ‡æ¢"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ¨¡å‹ç®¡ç†å™¨"""
        config = get_config()
        model_switch_config = config.get("model_switch", {})
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.api_llm = None  # API æ¨¡å‹ï¼ˆDeepSeekï¼‰
        self.local_llm = None  # æœ¬åœ°æ¨¡å‹ï¼ˆOllamaï¼‰- æš‚æ—¶ç¦ç”¨
        
        # åˆå§‹åŒ– API æ¨¡å‹
        try:
            api_config = config.get_model_config("api")
            if api_config.get('api_key'):
                self.api_llm = DeepSeekLLM(api_config)
                logger.info("API æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.warning(f"API æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
        
        # ========================================================================
        # Ollama æœ¬åœ°æ¨¡å‹æš‚æ—¶ç¦ç”¨ï¼ˆé…ç½®ä¿ç•™ï¼Œä½†ä¸åˆå§‹åŒ–ï¼‰
        # ========================================================================
        # try:
        #     local_config = config.get_model_config("local")
        #     self.local_llm = LocalLLM(local_config)
        # except Exception as e:
        #     logger.warning(f"æœ¬åœ°æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
        
        # åˆ‡æ¢ç­–ç•¥
        self.priority = model_switch_config.get('priority', 'api')  # é»˜è®¤ä½¿ç”¨ API
        self.fallback_to_api = model_switch_config.get('fallback_to_api', False)  # local å¤±è´¥æ—¶å›é€€åˆ° api
        
        # å½“å‰ä½¿ç”¨çš„æ¨¡å‹
        self._current_model: Optional[BaseLLM] = None
        self._select_model()
        
        logger.info(f"æ¨¡å‹ç®¡ç†å™¨å·²åˆå§‹åŒ–ï¼Œå½“å‰æ¨¡å‹: {self._current_model.__class__.__name__ if self._current_model else 'None'}")
    
    def _select_model(self):
        """æ ¹æ®ä¼˜å…ˆçº§é€‰æ‹©æ¨¡å‹"""
        # ========================================================================
        # æ¨¡å‹é€‰æ‹©é€»è¾‘ï¼šæ”¯æŒ api/localï¼Œä½† local æš‚æ—¶ç¦ç”¨
        # ========================================================================
        if self.priority == 'local':
            # local æš‚æ—¶ç¦ç”¨
            logger.warning("âš ï¸ æœ¬åœ°æ¨¡å‹ï¼ˆOllamaï¼‰æš‚æ—¶ç¦ç”¨ï¼Œåˆ‡æ¢åˆ° API")
            if self.api_llm:
                if self.api_llm.is_available():
                    self._current_model = self.api_llm
                    logger.info("âœ… å·²é€‰æ‹© API æ¨¡å‹ï¼ˆDeepSeekï¼‰")
                else:
                    self._current_model = None
                    logger.warning("âš ï¸ API æ¨¡å‹ä¸å¯ç”¨ï¼ˆå¥åº·æ£€æŸ¥å¤±è´¥ï¼‰")
                    logger.info("ğŸ’¡ æç¤º: è¯·æ£€æŸ¥ DEEPSEEK_API_KEY ç¯å¢ƒå˜é‡æ˜¯å¦æ­£ç¡®è®¾ç½®")
            else:
                self._current_model = None
                logger.warning("âš ï¸ API æ¨¡å‹æœªåˆå§‹åŒ–")
                logger.info("ğŸ’¡ æç¤º: è¯·æ£€æŸ¥ config.yaml ä¸­çš„ models.api.api_key é…ç½®")
        elif self.priority == 'api':
            # ä¼˜å…ˆä½¿ç”¨ API
            if self.api_llm:
                if self.api_llm.is_available():
                    self._current_model = self.api_llm
                    logger.info("âœ… å·²é€‰æ‹© API æ¨¡å‹ï¼ˆDeepSeekï¼‰")
                else:
                    self._current_model = None
                    logger.warning("âš ï¸ API æ¨¡å‹ä¸å¯ç”¨ï¼ˆå¥åº·æ£€æŸ¥å¤±è´¥ï¼‰")
                    logger.info("ğŸ’¡ æç¤º: è¯·æ£€æŸ¥ DEEPSEEK_API_KEY ç¯å¢ƒå˜é‡æ˜¯å¦æ­£ç¡®è®¾ç½®")
            else:
                self._current_model = None
                logger.warning("âš ï¸ API æ¨¡å‹æœªåˆå§‹åŒ–")
                logger.info("ğŸ’¡ æç¤º: è¯·æ£€æŸ¥ config.yaml ä¸­çš„ models.api.api_key é…ç½®")
        else:
            # æœªçŸ¥ä¼˜å…ˆçº§ï¼Œé»˜è®¤ä½¿ç”¨ API
            logger.warning(f"æœªçŸ¥çš„ä¼˜å…ˆçº§é…ç½®: {self.priority}ï¼Œä½¿ç”¨ API")
            if self.api_llm and self.api_llm.is_available():
                self._current_model = self.api_llm
                logger.info("å·²é€‰æ‹© API æ¨¡å‹ï¼ˆDeepSeekï¼‰")
            else:
                self._current_model = None
                logger.warning("æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹")
    
    def get_model(self) -> Optional[BaseLLM]:
        """
        è·å–å½“å‰æ¨¡å‹
        
        Returns:
            å½“å‰å¯ç”¨çš„æ¨¡å‹å®ä¾‹
        """
        # æ£€æŸ¥å½“å‰æ¨¡å‹æ˜¯å¦ä»ç„¶å¯ç”¨
        if self._current_model and self._current_model.is_available():
            return self._current_model
        
        # å¦‚æœå½“å‰æ¨¡å‹ä¸å¯ç”¨ï¼Œå°è¯•åˆ‡æ¢
        self._select_model()
        return self._current_model
    
    def chat_completion(
        self,
        messages: list,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        stream: bool = False
    ):
        """
        è°ƒç”¨æ¨¡å‹è¿›è¡ŒèŠå¤©è¡¥å…¨ï¼Œæ”¯æŒè‡ªåŠ¨å›é€€
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§ token æ•°
            stream: æ˜¯å¦æµå¼è¿”å›
            
        Returns:
            API å“åº”å¯¹è±¡æˆ–æµå¼è¿­ä»£å™¨
            
        Raises:
            RuntimeError: æ‰€æœ‰æ¨¡å‹éƒ½ä¸å¯ç”¨æ—¶æŠ›å‡ºå¼‚å¸¸
        """
        model = self.get_model()
        
        if model is None:
            raise RuntimeError("æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹")
        
        try:
            return model.chat_completion(
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
                stream=stream
            )
        except Exception as e:
            logger.error(f"æ¨¡å‹è°ƒç”¨å¤±è´¥: {e}")
            
            # ====================================================================
            # å›é€€é€»è¾‘ï¼šlocal å¤±è´¥æ—¶å›é€€åˆ° apiï¼ˆå¦‚æœå¯ç”¨ï¼‰
            # ====================================================================
            if self.fallback_to_api and isinstance(model, LocalLLM) and self.api_llm:
                logger.info("å°è¯•å›é€€åˆ° API æ¨¡å‹")
                try:
                    self._current_model = self.api_llm
                    return self.api_llm.chat_completion(
                        messages=messages,
                        temperature=temperature,
                        max_tokens=max_tokens,
                        stream=stream
                    )
                except Exception as e2:
                    logger.error(f"API æ¨¡å‹å›é€€ä¹Ÿå¤±è´¥: {e2}")
            
            raise
    
    def switch_model(self, model_type: str) -> bool:
        """
        æ‰‹åŠ¨åˆ‡æ¢æ¨¡å‹
        
        Args:
            model_type: æ¨¡å‹ç±»å‹ ('api' æˆ– 'local'ï¼Œlocal æš‚æ—¶ç¦ç”¨)
            
        Returns:
            æ˜¯å¦åˆ‡æ¢æˆåŠŸ
        """
        if model_type == 'local':
            # local æš‚æ—¶ç¦ç”¨
            logger.warning("æœ¬åœ°æ¨¡å‹ï¼ˆOllamaï¼‰æš‚æ—¶ç¦ç”¨ï¼Œæ— æ³•åˆ‡æ¢")
            return False
        
        if model_type == 'api':
            if self.api_llm and self.api_llm.is_available():
                self._current_model = self.api_llm
                logger.info("å·²åˆ‡æ¢åˆ° API æ¨¡å‹ï¼ˆDeepSeekï¼‰")
                return True
            else:
                logger.warning("æ— æ³•åˆ‡æ¢åˆ° APIï¼Œæ¨¡å‹ä¸å¯ç”¨")
                return False
        else:
            logger.warning(f"æœªçŸ¥çš„æ¨¡å‹ç±»å‹: {model_type}ï¼Œæ”¯æŒçš„é€‰é¡¹: 'api', 'local'")
            return False
    
    def get_current_model_type(self) -> str:
        """è·å–å½“å‰æ¨¡å‹ç±»å‹"""
        if isinstance(self._current_model, LocalLLM):
            return 'local'
        elif isinstance(self._current_model, DeepSeekLLM):
            return 'api'  # DeepSeekLLM å¯¹åº” api é…ç½®
        else:
            return 'none'

