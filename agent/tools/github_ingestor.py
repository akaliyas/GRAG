"""
GitHub æ–‡æ¡£æå–å·¥å…·
åŸºäº GitHub API æå–ä»“åº“æ–‡æ¡£ï¼Œæ”¯æŒ Markdown å’Œ Jupyter Notebook
å®Œå…¨å»çˆ¬è™«åŒ–ï¼Œä»…ä½¿ç”¨ç»“æ„åŒ–æ•°æ®æºï¼ˆSource Code is Truthï¼‰
"""
import logging
import os
import re
import time
from tqdm import tqdm
from typing import List, Dict, Optional, Tuple, Any
from urllib.parse import urlparse

try:
    import nbformat
    from nbformat import NotebookNode
except ImportError:
    nbformat = None
    NotebookNode = None

from dotenv import load_dotenv
from github import Github
from github.GithubException import (
    GithubException,
    RateLimitExceededException,
    UnknownObjectException,
    BadCredentialsException
)

from utils.schema import IngestionBatch, RawDoc

logger = logging.getLogger(__name__)

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()


class GitHubIngestor:
    """
    GitHub æ–‡æ¡£æå–å·¥å…·
    
    åŠŸèƒ½ï¼š
    - ä» GitHub ä»“åº“æå– Markdown å’Œ Jupyter Notebook æ–‡ä»¶
    - æ¸…æ´— .ipynb æ–‡ä»¶ï¼ˆä»…ä¿ç•™ Markdown å’Œ Code è¾“å…¥ï¼Œä¸¢å¼ƒ Outputï¼‰
    - æå– .md æ–‡ä»¶çš„ Frontmatter
    - ä¿®å¤ç›¸å¯¹é“¾æ¥ä¸º GitHub Raw URL
    """
    
    def __init__(self, github_token: Optional[str] = None):
        """
        åˆå§‹åŒ– GitHub æå–å·¥å…·
        
        Args:
            github_token: GitHub Personal Access Tokenï¼ˆä»ç¯å¢ƒå˜é‡è¯»å–ï¼Œå¯é€‰ï¼‰
        """
        token = github_token or os.getenv("GITHUB_TOKEN")
        if not token:
            logger.warning("GITHUB_TOKEN æœªè®¾ç½®ï¼Œå°†ä½¿ç”¨åŒ¿åè®¿é—®ï¼ˆé€Ÿç‡é™åˆ¶è¾ƒä½ï¼‰")
        
        try:
            self.github = Github(token) if token else Github()
            self.github_token = token
            logger.info("GitHub æå–å·¥å…·å·²åˆå§‹åŒ–")
        except BadCredentialsException:
            logger.error("GitHub è®¤è¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥ token æ˜¯å¦æœ‰æ•ˆ")
            raise
        except Exception as e:
            logger.error(f"åˆå§‹åŒ– GitHub å®¢æˆ·ç«¯å¤±è´¥: {e}")
            raise
    
    def _parse_github_url(self, url: str) -> Tuple[str, str, str]:
        """
        è§£æ GitHub URL
        
        Args:
            url: GitHub ä»“åº“ URLï¼Œå¦‚ https://github.com/openai/openai-python
            
        Returns:
            (owner, repo, path) å…ƒç»„
        """
        parsed = urlparse(url)
        path_parts = parsed.path.strip('/').split('/')
        
        if len(path_parts) < 2:
            raise ValueError(f"æ— æ•ˆçš„ GitHub URL: {url}")
        
        owner = path_parts[0]
        repo = path_parts[1]
        path = '/'.join(path_parts[2:]) if len(path_parts) > 2 else ''
        
        return owner, repo, path
    
    def _extract_frontmatter(self, content: str) -> Tuple[Dict[str, str], str]:
        """
        æå– Markdown æ–‡ä»¶çš„ Frontmatterï¼ˆYAML æ ¼å¼ï¼‰
        
        Args:
            content: Markdown æ–‡ä»¶å†…å®¹
            
        Returns:
            (frontmatter_dict, body_content) å…ƒç»„
        """
        frontmatter = {}
        body = content
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ Frontmatterï¼ˆä»¥ --- å¼€å¤´ï¼‰
        if content.startswith('---'):
            parts = content.split('---', 2)
            if len(parts) >= 3:
                frontmatter_text = parts[1].strip()
                body = parts[2].strip()
                
                # ç®€å•è§£æ YAMLï¼ˆä»…æ”¯æŒ key: value æ ¼å¼ï¼‰
                for line in frontmatter_text.split('\n'):
                    if ':' in line:
                        key, value = line.split(':', 1)
                        frontmatter[key.strip()] = value.strip().strip('"').strip("'")
        
        return frontmatter, body
    
    def _clean_notebook(self, notebook_content: str, repo_url: str) -> str:
        """
        æ¸…æ´— Jupyter Notebook æ–‡ä»¶
        
        ä»…ä¿ç•™ï¼š
        - Markdown å•å…ƒæ ¼
        - Code å•å…ƒæ ¼çš„è¾“å…¥éƒ¨åˆ†
        
        ä¸¢å¼ƒï¼š
        - Code å•å…ƒæ ¼çš„è¾“å‡ºï¼ˆåŒ…å« Base64 å›¾ç‰‡ã€é”™è¯¯ä¿¡æ¯ç­‰å™ªéŸ³ï¼‰
        - HTML æ ‡ç­¾ï¼ˆç‰¹åˆ«æ˜¯ tfo-notebook-buttons ç­‰å¯¼èˆªå…ƒç´ ï¼‰
        - å…¶ä»–å…ƒæ•°æ®
        
        Args:
            notebook_content: Notebook JSON å­—ç¬¦ä¸²
            repo_url: ä»“åº“ URLï¼ˆç”¨äºä¿®å¤é“¾æ¥ï¼‰
            
        Returns:
            æ¸…æ´—åçš„çº¯æ–‡æœ¬å†…å®¹
        """
        if nbformat is None:
            logger.warning("nbformat æœªå®‰è£…ï¼Œæ— æ³•å¤„ç† .ipynb æ–‡ä»¶")
            return notebook_content
        
        try:
            notebook = nbformat.reads(notebook_content, as_version=4)
            # è§„èŒƒåŒ– Notebookï¼ˆæ·»åŠ ç¼ºå¤±çš„ id å­—æ®µï¼Œæ¶ˆé™¤è­¦å‘Šï¼‰
            # nbformat 5.1.4+ æ”¯æŒ normalizeï¼Œæ—§ç‰ˆæœ¬ä¼šå¿½ç•¥
            try:
                # å°è¯•ä½¿ç”¨ normalize æ–¹æ³•ï¼ˆnbformat 5.1.4+ï¼‰
                if hasattr(nbformat, 'normalize'):
                    nbformat.normalize(notebook)
                else:
                    # æ—§ç‰ˆæœ¬æ‰‹åŠ¨æ·»åŠ  id å­—æ®µ
                    import uuid
                    for cell in notebook.cells:
                        if not hasattr(cell, 'id') or not cell.id:
                            cell.id = str(uuid.uuid4())
            except (AttributeError, TypeError, ImportError) as e:
                # å¦‚æœ normalize ä¸å¯ç”¨æˆ–å‡ºé”™ï¼Œè®°å½•è­¦å‘Šï¼Œä¸å½±å“åŠŸèƒ½
                logger.warning(f"nbformat.normalize ä¸å¯ç”¨æˆ–å¤„ç† notebook è§„èŒƒåŒ–å‡ºé”™: {e}")
        except Exception as e:
            logger.error(f"è§£æ Notebook å¤±è´¥: {e}")
            return notebook_content
        
        cleaned_parts = []
        
        for cell in notebook.cells:
            if cell.cell_type == 'markdown':
                # ä¿ç•™ Markdown å•å…ƒæ ¼
                cell_content = cell.source
                # ä¿®å¤ç›¸å¯¹é“¾æ¥
                cell_content = self._fix_relative_links(cell_content, repo_url)
                # æ¸…ç† HTML æ ‡ç­¾
                cell_content = self._clean_html_tags(cell_content)
                cleaned_parts.append(cell_content)
            elif cell.cell_type == 'code':
                # ä»…ä¿ç•™ä»£ç è¾“å…¥ï¼Œä¸¢å¼ƒè¾“å‡º
                code_input = cell.source
                # æ·»åŠ ä»£ç å—æ ‡è®°
                cleaned_parts.append(f"```python\n{code_input}\n```")
        
        cleaned_content = '\n\n'.join(cleaned_parts)
        
        # æœ€ç»ˆæ¸…ç†ï¼šç§»é™¤æ®‹ç•™çš„ HTML æ ‡ç­¾
        cleaned_content = self._clean_html_tags(cleaned_content)
        
        return cleaned_content
    
    def _clean_html_tags(self, content: str) -> str:
        """
        æ¸…ç† HTML æ ‡ç­¾ï¼Œä¿ç•™æ–‡æœ¬å†…å®¹
        
        ç­–ç•¥ï¼š
        1. å®Œå…¨ç§»é™¤ tfo-notebook-buttons è¡¨æ ¼ï¼ˆGoogle Colab å¯¼èˆªæŒ‰é’®ï¼‰
        2. ç§»é™¤å…¶ä»– HTML æ ‡ç­¾ï¼Œä¿ç•™æ–‡æœ¬å†…å®¹
        3. ä¿ç•™ Markdown ä»£ç å—ï¼ˆé¿å…è¯¯åˆ ï¼‰
        
        Args:
            content: åŒ…å« HTML çš„æ–‡æœ¬å†…å®¹
            
        Returns:
            æ¸…ç†åçš„çº¯æ–‡æœ¬å†…å®¹
        """
        # æ­¥éª¤ 1: ç§»é™¤ tfo-notebook-buttons è¡¨æ ¼ï¼ˆå®Œå…¨ç§»é™¤ï¼ŒåŒ…æ‹¬å†…å®¹ï¼‰
        # ä½¿ç”¨éè´ªå©ªåŒ¹é…ï¼ŒåŒ¹é…æ•´ä¸ªè¡¨æ ¼
        tfo_table_pattern = r'<table[^>]*class=["\']tfo-notebook-buttons["\'][^>]*>.*?</table>'
        content = re.sub(tfo_table_pattern, '', content, flags=re.DOTALL | re.IGNORECASE)
        
        # æ­¥éª¤ 2: ä¿æŠ¤ Markdown ä»£ç å—ï¼ˆé¿å…è¯¯åˆ ä»£ç å—å†…çš„ HTMLï¼‰
        # ä¸´æ—¶æ›¿æ¢ä»£ç å—ä¸ºå ä½ç¬¦
        code_blocks = []
        code_block_pattern = r'```[\s\S]*?```'
        
        def replace_code_block(match):
            placeholder = f"__CODE_BLOCK_{len(code_blocks)}__"
            code_blocks.append(match.group(0))
            return placeholder
        
        content = re.sub(code_block_pattern, replace_code_block, content)
        
        # æ­¥éª¤ 3: ç§»é™¤æ‰€æœ‰ HTML æ ‡ç­¾ï¼ˆä¿ç•™æ–‡æœ¬å†…å®¹ï¼‰
        # åŒ¹é… <tag> æˆ– <tag attr="..."> æ ¼å¼
        html_tag_pattern = r'<[^>]+>'
        content = re.sub(html_tag_pattern, '', content)
        
        # æ­¥éª¤ 4: æ¢å¤ä»£ç å—
        for i, code_block in enumerate(code_blocks):
            content = content.replace(f"__CODE_BLOCK_{i}__", code_block)
        
        # æ­¥éª¤ 5: æ¸…ç†å¤šä½™çš„ç©ºç™½è¡Œï¼ˆHTML æ ‡ç­¾ç§»é™¤åå¯èƒ½äº§ç”Ÿï¼‰
        content = re.sub(r'\n{3,}', '\n\n', content)
        
        # æ­¥éª¤ 6: æ¸…ç†è¡Œé¦–è¡Œå°¾ç©ºç™½
        content = content.strip()
        
        return content
    
    def _fix_relative_links(self, content: str, repo_url: str) -> str:
        """
        ä¿®å¤ Markdown ä¸­çš„ç›¸å¯¹é“¾æ¥ä¸º GitHub Raw URL
        
        Args:
            content: Markdown å†…å®¹
            repo_url: ä»“åº“ URL
            
        Returns:
            ä¿®å¤åçš„å†…å®¹
        """
        # è§£æä»“åº“ä¿¡æ¯
        owner, repo, _ = self._parse_github_url(repo_url)
        base_raw_url = f"https://raw.githubusercontent.com/{owner}/{repo}/main"
        
        # åŒ¹é…ç›¸å¯¹é“¾æ¥æ¨¡å¼ [text](../path/to/file)
        def replace_link(match):
            link_text = match.group(1)
            relative_path = match.group(2)
            # ç®€åŒ–å¤„ç†ï¼šå°†ç›¸å¯¹è·¯å¾„è½¬æ¢ä¸ºç»å¯¹è·¯å¾„
            absolute_path = relative_path.lstrip('./')
            return f"[{link_text}]({base_raw_url}/{absolute_path})"
        
        # æ›¿æ¢ç›¸å¯¹é“¾æ¥
        pattern = r'\[([^\]]+)\]\(([^)]+)\)'
        content = re.sub(pattern, replace_link, content)
        
        return content
    
    def fetch_repo_structure(
        self,
        repo_url: str,
        include_paths: Optional[List[str]] = None,
        exclude_paths: Optional[List[str]] = None,
        file_extensions: Optional[List[str]] = None,
        current_path: str = ""
    ) -> List[Dict[str, str]]:
        """
        EDA ç¬¬ä¸€æ­¥ï¼šè·å–ä»“åº“ç»“æ„ï¼ˆä¸ä¸‹è½½å†…å®¹ï¼‰
        
        Args:
            repo_url: GitHub ä»“åº“ URL
            include_paths: åŒ…å«çš„è·¯å¾„å‰ç¼€åˆ—è¡¨
            exclude_paths: æ’é™¤çš„è·¯å¾„å‰ç¼€åˆ—è¡¨
            file_extensions: æ–‡ä»¶æ‰©å±•ååˆ—è¡¨ï¼ˆé»˜è®¤ ['.md', '.ipynb']ï¼‰
            
        Returns:
            æ–‡ä»¶å…ƒæ•°æ®åˆ—è¡¨ï¼ŒåŒ…å« path, name, size, url ç­‰
        """
        if file_extensions is None:
            file_extensions = ['.md', '.mdx', '.ipynb', '.txt']
        
        if exclude_paths is None:
            exclude_paths = [
                '.github/', '.git/', '__pycache__/', 'node_modules/',
                '.vscode/', '.devcontainer/', '.inline-snapshot/', 'bin/',
                '_build/', 'legacy/', 'translations/'
            ]
        
        owner, repo, url_path = self._parse_github_url(repo_url)
        
        # ä½¿ç”¨ current_path ä½œä¸ºæœç´¢è·¯å¾„ï¼ˆé€’å½’æ—¶ä½¿ç”¨ï¼‰
        # å¦‚æœæ˜¯é¦–æ¬¡è°ƒç”¨ä¸” URL ä¸­åŒ…å«è·¯å¾„ï¼Œä½¿ç”¨ URL ä¸­çš„è·¯å¾„
        if current_path == "" and url_path:
            search_path = url_path
        else:
            search_path = current_path
        
        try:
            repo_obj = self.github.get_repo(f"{owner}/{repo}")
            
            # try to get tqdm
            if current_path == "":
                logger.info(f"å¼€å§‹æ‰«æä»“åº“ç»“æ„ï¼š{owner}/{repo}")
                start_time = time.time()
            # è·å–ç›®å½•å†…å®¹
            try:
                contents = repo_obj.get_contents(search_path)
            except UnknownObjectException:
                logger.warning(f"è·¯å¾„ä¸å­˜åœ¨: {search_path}")
                return []
            
            if not isinstance(contents, list):
                contents = [contents]
            
            files = []
            
            # try to get tqdm
            contents_iter = tqdm(contents, desc="æ‰«æä»“åº“æ–‡ä»¶",unit="é¡¹",disable=(current_path != ""))if current_path == "" else contents

            for content in contents_iter:
                if content.type == "file":
                    file_path = content.path
                    
                    # æ£€æŸ¥æ–‡ä»¶æ‰©å±•å
                    if not any(file_path.endswith(ext) for ext in file_extensions):
                        continue
                    
                    # æ£€æŸ¥æ’é™¤è·¯å¾„
                    if any(file_path.startswith(exclude) for exclude in exclude_paths):
                        continue
                    
                    # æ£€æŸ¥åŒ…å«è·¯å¾„
                    if include_paths and not any(
                        file_path.startswith(include) for include in include_paths
                    ):
                        continue
                    
                    # è¿‡æ»¤æ–‡ä»¶å¤§å°ï¼ˆ1KB - 100KBï¼‰
                    if content.size < 1024 or content.size > 102400:
                        logger.debug(f"æ–‡ä»¶ {file_path} å¤§å° {content.size} è¶…å‡ºèŒƒå›´ï¼Œè·³è¿‡")
                        continue
                    
                    files.append({
                        'path': file_path,
                        'name': content.name,
                        'size': content.size,
                        'sha': content.sha,
                        'url': content.html_url,
                        'download_url': content.download_url
                    })
                elif content.type == "dir":
                    # é€’å½’è·å–å­ç›®å½•
                    # æ£€æŸ¥æ˜¯å¦éœ€è¦é€’å½’ï¼ˆå¦‚æœè®¾ç½®äº† include_pathsï¼Œåªé€’å½’åŒ¹é…çš„ç›®å½•ï¼‰
                    should_recurse = True
                    if include_paths:
                        # æ£€æŸ¥å­ç›®å½•è·¯å¾„æ˜¯å¦åŒ¹é…ä»»ä½• include_paths
                        should_recurse = any(
                            content.path.startswith(inc) or inc.startswith(content.path)
                            for inc in include_paths
                        )
                    
                    if should_recurse:
                        sub_files = self.fetch_repo_structure(
                            repo_url,
                            include_paths,
                            exclude_paths,
                            file_extensions,
                            current_path=content.path  # ä½¿ç”¨å­ç›®å½•è·¯å¾„ä½œä¸ºæ–°çš„æœç´¢è·¯å¾„
                        )
                        files.extend(sub_files)
            
            return files
            
        except GithubException as e:
            logger.error(f"è·å–ä»“åº“ç»“æ„å¤±è´¥: {e}")
            return []
    
    def _download_raw_content(
        self,
        repo_url: str,
        file_paths: List[str]
    ) -> List[Dict[str, Any]]:
        """
        ä¸‹è½½åŸå§‹æ–‡ä»¶å†…å®¹ï¼ˆä¸åšä»»ä½•æ¸…æ´—ï¼‰
        
        Args:
            repo_url: GitHub ä»“åº“ URL
            file_paths: æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            
        Returns:
            åŸå§‹æ–‡æ¡£åˆ—è¡¨ï¼Œæ¯ä¸ªæ–‡æ¡£åŒ…å«ï¼špath, content, source_url, file_type, metadata
        """
        owner, repo, _ = self._parse_github_url(repo_url)
        documents = []
        
        for file_path in file_paths:
            try:
                # è·å–æ–‡ä»¶å†…å®¹
                repo_obj = self.github.get_repo(f"{owner}/{repo}")
                content_obj = repo_obj.get_contents(file_path)
                
                # è§£ç å†…å®¹ï¼ˆåŸå§‹ï¼Œä¸åšæ¸…æ´—ï¼‰
                if content_obj.encoding == "base64":
                    raw_content = content_obj.decoded_content.decode('utf-8')
                else:
                    raw_content = content_obj.decoded_content.decode('utf-8', errors='ignore')
                
                # ç¡®å®šæ–‡ä»¶ç±»å‹
                if file_path.endswith('.ipynb'):
                    file_type = 'notebook'
                elif file_path.endswith(('.md', '.mdx')):
                    file_type = 'markdown'
                else:
                    file_type = 'text'
                
                # æå– Frontmatterï¼ˆä»…æå–ï¼Œä¸ä¿®æ”¹å†…å®¹ï¼‰
                frontmatter = {}
                if file_type == 'markdown':
                    frontmatter, _ = self._extract_frontmatter(raw_content)
                
                # æ„å»º GitHub Raw URL
                source_url = f"https://raw.githubusercontent.com/{owner}/{repo}/main/{file_path}"
                
                documents.append({
                    'path': file_path,
                    'content': raw_content,  # åŸå§‹å†…å®¹ï¼Œä¸åšæ¸…æ´—
                    'source_url': source_url,
                    'file_type': file_type,
                    'metadata': {
                        'type': file_type,
                        'url': content_obj.html_url,
                        'frontmatter': frontmatter
                    }
                })
                
                logger.debug(f"å·²ä¸‹è½½åŸå§‹æ–‡ä»¶: {file_path}")
                
            except Exception as e:
                logger.error(f"ä¸‹è½½æ–‡ä»¶å¤±è´¥ ({file_path}): {e}")
                continue
        
        logger.info(f"æˆåŠŸä¸‹è½½ {len(documents)} ä¸ªåŸå§‹æ–‡ä»¶")
        return documents
    
    def extract_repo_docs(
        self,
        repo_url: str,
        file_extensions: Optional[List[str]] = None,
        max_files: Optional[int] = None,
        include_paths: Optional[List[str]] = None,
        exclude_paths: Optional[List[str]] = None
    ) -> IngestionBatch:
        """
        ä» GitHub ä»“åº“æå–æ–‡æ¡£ï¼Œè¿”å› IngestionBatch å¯¹è±¡ï¼ˆRaw Artifactï¼‰
        
        æ³¨æ„ï¼šæ­¤æ–¹æ³•è¿”å›åŸå§‹å†…å®¹ï¼Œä¸åšä»»ä½•æ¸…æ´—ã€‚æ¸…æ´—åº”åœ¨åç»­æ­¥éª¤å®Œæˆã€‚
        
        Args:
            repo_url: GitHub ä»“åº“ URL
            file_extensions: æ–‡ä»¶æ‰©å±•ååˆ—è¡¨ï¼ˆé»˜è®¤ ['.md', '.ipynb']ï¼‰
            max_files: æœ€å¤§æ–‡ä»¶æ•°ï¼ˆå¯é€‰ï¼Œé»˜è®¤æ— é™åˆ¶ï¼‰
            include_paths: åŒ…å«çš„è·¯å¾„å‰ç¼€åˆ—è¡¨
            exclude_paths: æ’é™¤çš„è·¯å¾„å‰ç¼€åˆ—è¡¨
            
        Returns:
            IngestionBatch å¯¹è±¡ï¼ˆRaw Artifactï¼‰
        """
        if file_extensions is None:
            file_extensions = ['.md', '.mdx', '.ipynb']
        
        # è·å–ä»“åº“ç»“æ„
        file_metadata = self.fetch_repo_structure(
            repo_url,
            include_paths=include_paths,
            exclude_paths=exclude_paths,
            file_extensions=file_extensions
        )
        
        # é™åˆ¶æ–‡ä»¶æ•°é‡
        if max_files:
            file_metadata = file_metadata[:max_files]
        
        # ä¸‹è½½åŸå§‹å†…å®¹ï¼ˆä¸åšæ¸…æ´—ï¼‰
        file_paths = [f['path'] for f in file_metadata]
        raw_documents = self._download_raw_content(repo_url, file_paths)
        
        # è½¬æ¢ä¸º RawDoc å¯¹è±¡
        raw_docs = []
        for doc in raw_documents:
            raw_doc = RawDoc(
                path=doc['path'],
                content=doc['content'],  # åŸå§‹å†…å®¹
                source_url=doc['source_url'],
                file_type=doc['file_type'],
                metadata=doc['metadata']
            )
            raw_docs.append(raw_doc)
        
        # ç”Ÿæˆ batch_idï¼ˆåŸºäº repo_url å’Œæå–æ—¶é—´ï¼‰
        import hashlib
        from datetime import datetime
        batch_id = hashlib.md5(f"{repo_url}_{datetime.now().isoformat()}".encode()).hexdigest()[:16]
        
        # åˆ›å»º IngestionBatch
        batch = IngestionBatch(
            batch_id=batch_id,
            repo_url=repo_url,
            docs=raw_docs
        )
        
        logger.info(f"æˆåŠŸæå– {len(raw_docs)} ä¸ªæ–‡æ¡£ï¼Œè¿”å› IngestionBatch å¯¹è±¡")
        return batch
    
    def download_and_clean(
        self,
        repo_url: str,
        file_paths: Optional[List[str]] = None,
        include_paths: Optional[List[str]] = None,
        exclude_paths: Optional[List[str]] = None
    ) -> List[Dict[str, str]]:
        """
        EDA ç¬¬ä¸‰æ­¥ï¼šä¸‹è½½å¹¶æ¸…æ´—æ–‡ä»¶ä¸º LightRAG å¯ç”¨çš„æ–‡æœ¬
        
        Args:
            repo_url: GitHub ä»“åº“ URL
            file_paths: è¦ä¸‹è½½çš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼ˆå¦‚æœä¸º Noneï¼Œåˆ™è‡ªåŠ¨å‘ç°ï¼‰
            include_paths: åŒ…å«çš„è·¯å¾„å‰ç¼€åˆ—è¡¨
            exclude_paths: æ’é™¤çš„è·¯å¾„å‰ç¼€åˆ—è¡¨
            
        Returns:
            æ¸…æ´—åçš„æ–‡æ¡£åˆ—è¡¨ï¼Œæ¯ä¸ªæ–‡æ¡£åŒ…å«ï¼š
            - content: æ¸…æ´—åçš„æ–‡æœ¬å†…å®¹
            - path: æ–‡ä»¶è·¯å¾„
            - metadata: å…ƒæ•°æ®ï¼ˆFrontmatterã€æ–‡ä»¶ä¿¡æ¯ç­‰ï¼‰
        """
        owner, repo, _ = self._parse_github_url(repo_url)
        
        # å¦‚æœæ²¡æœ‰æŒ‡å®šæ–‡ä»¶è·¯å¾„ï¼Œå…ˆè·å–ä»“åº“ç»“æ„
        if file_paths is None:
            file_metadata = self.fetch_repo_structure(
                repo_url,
                include_paths=include_paths,
                exclude_paths=exclude_paths
            )
            file_paths = [f['path'] for f in file_metadata]
        
        documents = []
        
        for file_path in file_paths:
            try:
                # è·å–æ–‡ä»¶å†…å®¹
                repo_obj = self.github.get_repo(f"{owner}/{repo}")
                content_obj = repo_obj.get_contents(file_path)
                
                # è§£ç å†…å®¹
                if content_obj.encoding == "base64":
                    raw_content = content_obj.decoded_content.decode('utf-8')
                else:
                    raw_content = content_obj.decoded_content.decode('utf-8', errors='ignore')
                
                # æ ¹æ®æ–‡ä»¶ç±»å‹æ¸…æ´—
                if file_path.endswith('.ipynb'):
                    cleaned_content = self._clean_notebook(raw_content, repo_url)
                    metadata = {
                        'type': 'notebook',
                        'path': file_path,
                        'url': content_obj.html_url
                    }
                elif file_path.endswith(('.md', '.mdx')):
                    frontmatter, body = self._extract_frontmatter(raw_content)
                    cleaned_content = self._fix_relative_links(body, repo_url)
                    metadata = {
                        'type': 'markdown',
                        'path': file_path,
                        'url': content_obj.html_url,
                        'frontmatter': frontmatter
                    }
                else:
                    # å…¶ä»–æ–‡æœ¬æ–‡ä»¶ç›´æ¥ä½¿ç”¨
                    cleaned_content = raw_content
                    metadata = {
                        'type': 'text',
                        'path': file_path,
                        'url': content_obj.html_url
                    }
                
                documents.append({
                    'content': cleaned_content,
                    'path': file_path,
                    'metadata': metadata
                })
                
                logger.debug(f"å·²å¤„ç†æ–‡ä»¶: {file_path}")
                
            except Exception as e:
                logger.error(f"å¤„ç†æ–‡ä»¶å¤±è´¥ ({file_path}): {e}")
                continue
        
        logger.info(f"æˆåŠŸå¤„ç† {len(documents)} ä¸ªæ–‡ä»¶")
        return documents

if __name__ == "__main__":
    """
    æµ‹è¯• GitHub æå–å·¥å…·
    
    æµ‹è¯•åœºæ™¯ï¼š
    1. è·å–ä»“åº“ç»“æ„ï¼ˆä¸ä¸‹è½½å†…å®¹ï¼‰
    2. ä¸‹è½½å¹¶æ¸…æ´—æ–‡æ¡£
    3. æµ‹è¯•ä¸åŒæ–‡ä»¶ç±»å‹ï¼ˆ.md, .ipynbï¼‰
    4. æµ‹è¯•è·¯å¾„è¿‡æ»¤
    """
    import sys
    
    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    ingestor = GitHubIngestor()
    
    # æµ‹è¯•ä»“åº“ URLï¼ˆå¯ä»¥æ ¹æ®éœ€è¦ä¿®æ”¹ï¼‰
    test_repo = "https://github.com/google/generative-ai-docs"
    test_path = "site/en/gemini-api"
    
    print("=" * 60)
    print("æµ‹è¯• 1: è·å–ä»“åº“ç»“æ„ï¼ˆä¸ä¸‹è½½å†…å®¹ï¼‰")
    print("=" * 60)
    
    try:
        files = ingestor.fetch_repo_structure(
            test_repo,
            include_paths=[test_path],
            file_extensions=['.md', '.ipynb']
        )
        print(f"âœ… æ‰¾åˆ° {len(files)} ä¸ªæ–‡ä»¶")
        print(f"\nå‰ 5 ä¸ªæ–‡ä»¶ç¤ºä¾‹:")
        for f in files[:5]:
            print(f"  - {f['path']} ({f['size']} bytes)")
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        sys.exit(1)
    
    print("\n" + "=" * 60)
    print("æµ‹è¯• 2: ä¸‹è½½å¹¶æ¸…æ´—æ–‡æ¡£ï¼ˆé™åˆ¶æ•°é‡ï¼‰")
    print("=" * 60)
    
    if not files:
        print("âš ï¸  æ²¡æœ‰æ‰¾åˆ°æ–‡ä»¶ï¼Œè·³è¿‡ä¸‹è½½æµ‹è¯•")
        print("=" * 60)
        print("âœ… æµ‹è¯•å®Œæˆï¼ˆéƒ¨åˆ†è·³è¿‡ï¼‰")
        print("=" * 60)
        sys.exit(0)
    
    try:
        # åªå¤„ç†å‰ 3 ä¸ªæ–‡ä»¶ï¼ˆå¿«é€Ÿæµ‹è¯•ï¼‰
        file_paths = [f['path'] for f in files[:3]]
        print(f"å‡†å¤‡ä¸‹è½½ {len(file_paths)} ä¸ªæ–‡ä»¶: {file_paths}")
        
        documents = ingestor.download_and_clean(
            test_repo,
            file_paths=file_paths
        )
        
        print(f"âœ… æˆåŠŸå¤„ç† {len(documents)} ä¸ªæ–‡æ¡£\n")
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_chars = 0
        type_counts = {}
        
        for i, doc in enumerate(documents, 1):
            doc_type = doc['metadata']['type']
            type_counts[doc_type] = type_counts.get(doc_type, 0) + 1
            total_chars += len(doc['content'])
            
            print(f"æ–‡æ¡£ {i}: {doc['path']}")
            print(f"  ç±»å‹: {doc_type}")
            print(f"  å†…å®¹é•¿åº¦: {len(doc['content']):,} å­—ç¬¦")
            print(f"  GitHub URL: {doc['metadata']['url']}")
            
            if doc_type == 'markdown' and 'frontmatter' in doc['metadata']:
                frontmatter = doc['metadata']['frontmatter']
                if frontmatter:
                    print(f"  Frontmatter: {frontmatter}")
            
            # æ˜¾ç¤ºå†…å®¹é¢„è§ˆï¼ˆå‰ 150 å­—ç¬¦ï¼‰
            content_preview = doc['content'][:150].replace('\n', ' ').strip()
            print(f"  å†…å®¹é¢„è§ˆ: {content_preview}...")
            print()
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        print("=" * 60)
        print("ğŸ“Š å¤„ç†ç»“æœç»Ÿè®¡")
        print("=" * 60)
        print(f"æ€»æ–‡æ¡£æ•°: {len(documents)}")
        print(f"æ€»å­—ç¬¦æ•°: {total_chars:,}")
        print(f"æ–‡ä»¶ç±»å‹åˆ†å¸ƒ:")
        for doc_type, count in type_counts.items():
            print(f"  - {doc_type}: {count} ä¸ª")
        print()
        
        # è¯´æ˜æ•°æ®æ ¼å¼å’Œä½ç½®
        print("=" * 60)
        print("ğŸ“¦ æ•°æ®æ ¼å¼è¯´æ˜")
        print("=" * 60)
        print("å¤„ç†åçš„æ•°æ®æ ¼å¼ï¼š")
        print("  - æ•°æ®ç±»å‹: Python List[Dict]")
        print("  - æ¯ä¸ªæ–‡æ¡£åŒ…å«:")
        print("    â€¢ content: str - æ¸…æ´—åçš„æ–‡æœ¬å†…å®¹ï¼ˆçº¯æ–‡æœ¬ï¼‰")
        print("    â€¢ path: str - æ–‡ä»¶åœ¨ä»“åº“ä¸­çš„è·¯å¾„")
        print("    â€¢ metadata: Dict - å…ƒæ•°æ®ï¼ˆç±»å‹ã€URLã€Frontmatter ç­‰ï¼‰")
        print()
        print("å½“å‰çŠ¶æ€: æ•°æ®ä»…å­˜åœ¨äºå†…å­˜ä¸­ï¼Œæœªä¿å­˜åˆ°æ–‡ä»¶")
        print()
        
        # ä¿å­˜åˆ° JSON æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
        import json
        from pathlib import Path
        
        output_dir = Path("samples")
        output_dir.mkdir(exist_ok=True)
        
        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
        owner, repo, _ = ingestor._parse_github_url(test_repo)
        output_file = output_dir / f"{owner}_{repo}_cleaned_docs.json"
        
        # ä¿å­˜æ•°æ®
        output_data = {
            "source": "GitHub Repository",
            "repo_url": test_repo,
            "extracted_at": __import__('datetime').datetime.now().isoformat(),
            "total_documents": len(documents),
            "total_characters": total_chars,
            "type_distribution": type_counts,
            "documents": documents
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ æ•°æ®å·²ä¿å­˜åˆ°: {output_file}")
        print(f"   æ–‡ä»¶å¤§å°: {output_file.stat().st_size / 1024:.2f} KB")
        print()
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
    
    print("=" * 60)
    print("âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
    print("=" * 60)
    print()
    print("ğŸ“‹ ä¸‹ä¸€æ­¥æ“ä½œå»ºè®®ï¼š")
    print("=" * 60)
    print("1. ã€é›†æˆåˆ° LightRAGã€‘")
    print("   å°†æ¸…æ´—åçš„æ–‡æ¡£æ·»åŠ åˆ°çŸ¥è¯†åº“ï¼š")
    print("   ```python")
    print("   from knowledge.lightrag_wrapper import LightRAGWrapper")
    print("   from models.model_manager import ModelManager")
    print("   ")
    print("   model_manager = ModelManager()")
    print("   lightrag = LightRAGWrapper(model_manager)")
    print("   ")
    print("   # æå–æ–‡æ¡£å†…å®¹åˆ—è¡¨")
    print("   doc_contents = [doc['content'] for doc in documents]")
    print("   ")
    print("   # æ·»åŠ åˆ°çŸ¥è¯†åº“")
    print("   lightrag.add_documents(doc_contents)")
    print("   ```")
    print()
    print("2. ã€é€šè¿‡ Agent è°ƒç”¨ã€‘")
    print("   åœ¨ Agent å·¥ä½œæµä¸­ä½¿ç”¨ GitHubIngestorï¼š")
    print("   ```python")
    print("   from agent.tools.github_ingestor import GitHubIngestor")
    print("   ")
    print("   ingestor = GitHubIngestor()")
    print("   documents = ingestor.download_and_clean(repo_url)")
    print("   # Agent ä¼šè‡ªåŠ¨è°ƒç”¨ lightrag.add_documents()")
    print("   ```")
    print()
    print("3. ã€æ‰¹é‡å¤„ç†å¤šä¸ªä»“åº“ã€‘")
    print("   å¯ä»¥å¾ªç¯å¤„ç†å¤šä¸ª GitHub ä»“åº“ï¼Œç»Ÿä¸€æ·»åŠ åˆ°çŸ¥è¯†åº“")
    print()
    print("4. ã€éªŒè¯æ•°æ®è´¨é‡ã€‘")
    print("   æ£€æŸ¥ä¿å­˜çš„ JSON æ–‡ä»¶ï¼Œç¡®è®¤æ¸…æ´—æ•ˆæœæ˜¯å¦ç¬¦åˆé¢„æœŸ")
    print("=" * 60)
