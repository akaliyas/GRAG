# 基于知识图谱增强检索生成的技术文档智能问答系统

---

## 项目概述

### 1.1 项目背景

随着技术文档的快速增长，传统的文档检索方式已无法满足开发者快速获取准确信息的需求。本项目旨在构建一个基于知识图谱增强检索生成（GraphRAG）的智能问答系统，通过结合向量检索和图结构检索，实现对技术文档的深度理解和精准问答。

### 1.2 项目目标

- **主要目标**：构建一个支持技术文档智能问答的系统，能够准确理解用户问题并基于文档内容生成高质量答案
- **技术目标**：展示GraphRAG技术在文档问答中的应用，包括知识图谱构建、双层检索机制、智能问答生成等核心技术
- **应用目标**：支持OpenAI官方文档等技术文档的智能问答，帮助开发者快速获取准确的技术指导

### 1.3 项目范围

- **数据来源**：
  - GitHub 仓库文档（主要，已实现）：支持 Markdown 和 Jupyter Notebook
  - 已成功提取：OpenAI Cookbook（https://github.com/openai/openai-cookbook，256 个文档）
  - 未来扩展：支持其他 GitHub 仓库和文档网站（通过 MCP 服务或 API）
- **数据规模**：单个仓库约 200-400 个文档，总数据量约 200-500MB
- **应用场景**：目前以单文档查询为主，支持开发者针对技术文档的智能问答，GRAG 技术以支持将来多文档联查。

## 系统架构

### 2.1 整体架构

系统采用分层架构设计，包含以下核心层次：

```
┌─────────────────────────────────────────┐
│        应用层 (Application Layer)        │
│    FastAPI (后端) + Streamlit (前端)     │
└─────────────────────────────────────────┘
                    ↓
┌─────────────────────────────────────────┐
│         Agent层 (Agent Layer)           │
│         LangGraph (工作流编排)           │
└─────────────────────────────────────────┘
                    ↓
┌─────────────────────────────────────────┐
│         模型层 (Model Layer)             │
│   DeepSeek API / Llama-Factory LoRA     │
└─────────────────────────────────────────┘
                    ↓
┌─────────────────────────────────────────┐
│      知识存储层 (Knowledge Storage)       │
│  LightRAG (PostgreSQL + Neo4j 混合存储)  │
└─────────────────────────────────────────┘
                    ↓
┌─────────────────────────────────────────┐
│      数据处理层 (Data Processing)         │
│         LightRAG (知识图谱构建)           │
└─────────────────────────────────────────┘
                    ↓
┌─────────────────────────────────────────┐
│      数据采集层 (Data Collection)         │
│    GitHub API (GitHubIngestor)           │
│    Pipeline: Fetch -> Clean -> Ingest    │
│    未来扩展: MCP 服务 (Context7 等)      │
└─────────────────────────────────────────┘
```

### 2.2 核心流程

1. **数据提取（Fetch）**：使用 GitHub API 直接从仓库提取文档（Markdown/Notebook），生成 Raw Artifact
2. **数据清洗（Clean）**：清洗文档内容（Frontmatter 剥离、Notebook 清理、链接修复），生成 Clean Artifact
3. **知识构建（Ingest）**：通过 LightRAG 自动完成实体抽取、关系抽取、图构建和向量化
4. **知识存储**：将处理后的数据存储到 PostgreSQL（KV/向量）和 Neo4j（图）
5. **智能检索**：采用双层检索机制（全局检索+局部检索）获取相关上下文
6. **答案生成**：基于检索结果和知识图谱，使用 LLM 生成高质量答案

## 技术选型

### 3.1 核心技术栈

#### 3.1.1 数据处理层（优先采用LightRAG）

**主要方案：LightRAG**
- **选型理由**：
  - 成本优势：API调用次数减少约90%（3450-6900次 → 200-400次），成本从12-25元降至0.37-2.2元
  - 性能优势：双层检索机制，多数指标优于传统GraphRAG
  - 开发效率：使用现成框架，开发时间节省约50%
  - 增量更新：内置支持，符合每月更新需求

- **核心功能**：
  - 自动实体/关系抽取（批量处理，减少API调用）
  - 自动图构建和向量化
  - 双层检索（全局检索+局部检索）
  - 支持PostgreSQL和Neo4j作为存储后端

**备选方案：GraphRAG**
- 保留作为参考，适用于需要深度定制或处理复杂全局性问题的场景

#### 3.1.2 数据采集层

**主要方案：GitHub API 直接提取**

- **选型理由**：
  - **零风险**：使用官方 API，完全避免 IP 封禁和反爬虫问题
  - **结构化数据**：直接获取源代码，数据质量高（Source Code is Truth）
  - **成本低**：GitHub API 免费额度充足（5000 次/小时，有 Token）
  - **开发效率**：无需处理反爬虫逻辑，专注于数据清洗和知识构建
  - **合规性**：完全符合 GitHub 服务条款

- **技术方案**：
  - **GitHub API**：使用 PyGithub 库直接访问仓库内容
  - **支持格式**：Markdown (.md, .mdx)、Jupyter Notebook (.ipynb)
  - **自动清洗**：提取 Frontmatter、清理 Notebook 输出、修复相对链接
  - **Pipeline 模式**：Fetch -> Clean -> Ingest 三步流程，文件驱动，支持断点调试

- **已实现功能**：
  - ✅ GitHub 仓库文档提取（`GitHubIngestor`）
  - ✅ Pipeline 脚本（`pipeline_fetch.py`, `pipeline_clean.py`, `pipeline_ingest.py`）
  - ✅ 成功提取 OpenAI Cookbook（256 个文档）

**扩展方案：MCP 服务集成（未来规划）**

- **Context7 等 MCP 服务**：保留作为扩展方案，用于：
  - 访问非 GitHub 文档源（官方文档网站、API 文档等）
  - 通过 Model Context Protocol 统一接口访问多种文档源
  - 支持动态文档更新和实时查询
- **优势**：
  - 标准化接口，便于集成多种文档源
  - 支持 Agent 直接调用，无需预处理
  - 可扩展性强，支持未来新的文档源类型

**备选方案：SaaS 爬虫服务（特殊场景）**

- **Firecrawl / Jina Reader**：保留作为备选，用于：
  - 需要爬取非 GitHub 文档网站的场景
  - 处理需要 JavaScript 渲染的复杂页面
  - 特殊场景下的自定义需求
- **风险说明**：SaaS 服务依赖外部服务商，需要考虑可用性和成本

#### 3.1.3 知识存储层

- **存储架构**：混合存储架构
  - **PostgreSQL**：
    - KV 存储（`PGKVStorage`）：LLM 响应缓存、文本块、文档信息
    - 向量存储（`PGVectorStorage`）：实体向量、关系向量、块向量（使用 pgvector）
    - 文档状态存储（`PGDocStatusStorage`）：文档索引状态
  - **Neo4j**：
    - 图存储（`Neo4JStorage`）：实体关系图（相比 PostgreSQL AGE 有更好的性能）
    - 已配置并支持，可通过环境变量切换
- **缓存策略**：PostgreSQL 缓存表
  - 支持离线查询
  - 基于用户反馈的质量管理
  - LRU 清理策略

#### 3.1.4 模型层（双方案支持）

**方案一：DeepSeek API（主要方案）**
- **适用场景**：快速开发、成本控制、资源受限
- **优势**：
  - 成本低：约0.37-2.2元总成本（200-400页文档）
  - 性能稳定：DeepSeek-V3性能优秀
  - 无需本地部署：简化架构，降低资源需求
- **集成方式**：LightRAG支持DeepSeek API，可配置自定义LLM函数

**方案二：Llama-Factory LoRA微调（扩展方案）**
- **适用场景**：需要离线能力、追求极致效果、有GPU资源
- **技术方案**：
  - 基础模型：Qwen7b、ChatGLM等开源模型
  - 微调方式：LoRA（参数高效微调）
  - 微调目标：针对技术文档问答任务进行领域适配
  - 部署方式：本地部署，通过Ollama或vLLM提供服务
- **切换策略**：支持API和本地模型的动态切换，根据资源情况和需求选择

#### 3.1.5 应用层

- **后端框架**：FastAPI
  - 异步支持，性能高
  - 自动生成API文档
  - 类型提示支持好
- **前端框架**：Streamlit
  - 开发速度快
  - 适合数据应用和演示

#### 3.1.6 部署方案

- **部署方式**：Docker Compose
- **服务配置**：
  - PostgreSQL：KV 存储、向量存储、文档状态存储
  - Neo4j：图存储（已配置，可通过环境变量启用）
  - FastAPI：后端 API 服务
  - Streamlit：前端 Web 界面
- **资源需求**：约 1.2-1.6GB 内存（适合 16GB 设备）

### 3.2 技术优势

1. **成本控制**：LightRAG 方案大幅降低 API 调用成本（约 90% 节省）
2. **性能优化**：双层检索机制，检索效果优于传统 RAG
3. **开发效率**：使用成熟框架，快速迭代
4. **灵活扩展**：支持 API 和本地模型双方案，适应不同场景
5. **增量更新**：内置支持，便于文档定期更新
6. **零风险数据采集**：使用 GitHub API 完全避免反爬虫风险
7. **混合存储架构**：PostgreSQL + Neo4j，充分利用各数据库优势
8. **Pipeline 模式**：文件驱动，支持断点调试和人工介入

## 核心功能

### 4.1 数据采集功能

- **GitHub API 集成**：通过 GitHub API 直接提取仓库文档
  - 支持 Markdown (.md, .mdx) 和 Jupyter Notebook (.ipynb)
  - 自动处理相对链接，转换为 GitHub Raw URL
  - 提取 Frontmatter 元数据
  - 清洗 Notebook 输出，仅保留代码和 Markdown
- **Pipeline 流程**：
  - **Fetch**：从 GitHub 仓库提取原始文档（Raw Artifact）
  - **Clean**：清洗文档内容，生成清洗后的文档（Clean Artifact）
  - **Ingest**：导入到 LightRAG，构建知识图谱
- **多源支持**：
  - GitHub 仓库（主要，已实现）
  - 未来扩展：MCP 服务（Context7 等）支持其他文档源
  - 未来扩展：SaaS 服务（Firecrawl/Jina Reader）支持文档网站
- **数据清洗**：提取纯文本内容，去除无关元素，规范化格式
- **结构化存储**：将处理后的数据存储到 PostgreSQL 和 Neo4j

### 4.2 知识图谱构建

- 自动实体抽取（框架、库、API、概念等）
- 自动关系抽取（依赖、导入、使用等）
- 知识图谱构建和存储

### 4.3 智能检索

- 双层检索机制：
  - 全局检索：识别主题簇，宏观理解
  - 局部检索：聚焦具体实体，精准定位
- 向量检索和图查询融合

### 4.4 智能问答

- 基于检索结果和知识图谱生成答案
- 支持上下文理解和多轮对话
- 答案质量评估和反馈机制

### 4.5 用户反馈

- 支持用户对答案质量的反馈
- 基于反馈优化缓存质量
- 持续改进系统性能

## 实施计划

### 5.1 开发阶段

**Phase 1：基础搭建（已完成）**
- ✅ 环境配置（Docker Compose）
- ✅ GitHub API 集成（GitHubIngestor）
- ✅ Pipeline 流程实现（Fetch/Clean/Ingest）
- ✅ LightRAG 集成和测试
- ✅ Neo4j 图数据库配置
- ✅ 样本数据提取和验证（OpenAI Cookbook，256 个文档）

**Phase 2：核心功能（进行中）**
- ✅ 知识图谱构建
- 🔄 双层检索实现
- 🔄 API 服务开发

**Phase 3：应用开发（计划中）**
- ⏳ 前端界面开发
- ⏳ 用户反馈功能
- ⏳ 系统集成测试

**Phase 4：优化完善（计划中）**
- ⏳ 性能优化
- ⏳ 准确度优化
- ⏳ 文档编写

### 5.2 技术难点

1. **数据采集策略**：✅ 已解决，使用 GitHub API 完全避免反爬虫风险
2. **知识图谱构建**：🔄 进行中，确保实体和关系抽取的准确性，优化 prompt 设计
3. **双层检索融合**：🔄 进行中，优化全局检索和局部检索的结果融合策略
4. **模型切换**：✅ 已实现，支持 API 和本地模型的动态切换
5. **增量更新**：✅ 已支持，LightRAG 内置增量更新机制
6. **混合存储架构**：✅ 已实现，PostgreSQL + Neo4j 混合存储

### 5.3 风险评估

- **技术风险**：
  - LightRAG 框架的稳定性和兼容性（已评估，项目成熟度高）
  - Neo4j 连接稳定性（已配置健康检查，支持自动重连）
- **数据风险**：
  - 数据采集的合法性：✅ 已解决，使用 GitHub API 确保合规性
  - 数据质量：通过 Pipeline 清洗流程确保内容准确性
- **成本风险**：
  - API 调用成本（LightRAG 已优化，成本可控）
  - GitHub API 成本（免费额度充足，5000 次/小时）
- **运营风险**：
  - GitHub API 速率限制（通过 Token 提升限制，已处理）
  - 存储扩展性（混合架构支持水平扩展）

## 预期成果

### 6.1 系统功能

- ✅ 完成数据采集 Pipeline（Fetch/Clean/Ingest）
- ✅ 支持 GitHub 仓库文档提取（已成功提取 OpenAI Cookbook）
- 🔄 实现知识图谱增强的检索生成（进行中）
- ⏳ 支持技术文档的智能问答（计划中）

### 6.2 技术指标

- **检索准确率**：Comprehensiveness 49.6-54.8%，Diversity 59.2-77.2%
- **响应时间**：平均响应时间 < 2秒
- **成本控制**：API调用成本 < 2.2元（200-400页文档）

### 6.3 学术价值

- 展示GraphRAG技术在文档问答中的应用
- 验证LightRAG和传统GraphRAG的性能差异
- 探索知识图谱增强检索的优化方法

## 技术特色

### 7.1 创新点

1. **双层检索机制**：结合全局检索和局部检索，提升检索效果
2. **成本优化**：通过 LightRAG 大幅降低 API 调用成本
3. **双模型方案**：支持 API 和本地模型，适应不同场景
4. **增量更新**：支持文档的增量更新，无需全量重建
5. **零风险数据采集**：使用 GitHub API 完全避免反爬虫风险
6. **Pipeline 模式**：文件驱动，支持断点调试和人工介入
7. **混合存储架构**：PostgreSQL + Neo4j，充分利用各数据库优势

### 7.2 技术深度

- 知识图谱构建和查询
- 向量检索和图检索融合
- LLM微调和Prompt工程
- 分布式系统设计（Docker部署）

## 长期规划

### 8.1 短期目标（1-3个月）

**Phase 1：核心功能实现（进行中）**
- ✅ 完成 GitHub API 集成（GitHubIngestor）
- ✅ 实现 LightRAG 知识图谱构建
- 🔄 完成双层检索机制
- 🔄 开发基础问答功能

**Phase 2：系统优化（计划中）**
- 🔄 优化实体/关系抽取 prompt
- ✅ 实现增量更新机制（LightRAG 内置支持）
- 🔄 完善缓存策略
- 🔄 性能优化和监控

**Phase 3：扩展功能（计划中）**
- ⏳ MCP 服务集成（Context7 等）
- ⏳ 支持更多文档源类型
- ⏳ 知识图谱可视化

### 8.2 中期目标（3-6个月）

**功能扩展**
- 支持多文档源联合查询
- 实现知识图谱可视化界面
- 添加文档编辑和管理功能
- 支持用户自定义文档源

**技术深化**
- 探索更优的检索融合策略
- 优化知识图谱构建质量
- 实现模型微调（如需要）
- 提升问答准确率

### 8.3 长期目标（6-12个月）

**产品化**
- 完善前端 UI/UX
- 实现多租户支持
- 添加权限管理和安全机制
- 提供 API 服务

**技术演进**
- 探索更先进的 GraphRAG 技术
- 支持多模态文档（图片、表格等）
- 实现自动文档更新和同步
- 构建文档质量评估体系

### 8.4 技术债务管理

**数据采集策略**
- ✅ 短期：使用 GitHub API，零风险、高可靠性（已完成）
- 🔄 中期：集成 MCP 服务（Context7 等），支持更多文档源
- ⏳ 长期：根据实际需求选择最优方案（GitHub API / MCP / SaaS）

**模型策略**
- ✅ 短期：优先使用 DeepSeek API，成本低、效果好（已实现）
- ⏳ 中期：评估本地模型需求，如需要则实现
- ⏳ 长期：根据使用场景动态选择最优模型

**存储策略**
- ✅ 短期：混合存储架构（PostgreSQL + Neo4j）（已实现）
- 🔄 中期：优化存储性能，评估是否需要其他存储后端
- ⏳ 长期：根据数据规模选择最优存储方案

## 总结

本项目旨在构建一个基于知识图谱增强检索生成的技术文档智能问答系统，通过采用 LightRAG 框架实现低成本、高性能的文档问答能力。系统采用 GitHub API 完全避免反爬虫风险，支持 DeepSeek API 和本地模型双方案，能够适应不同的应用场景和资源条件。

**核心设计理念**：
- **风险最小化**：使用 GitHub API 完全避免反爬虫风险，零 IP 封禁风险
- **成本优化**：通过 LightRAG 和合理的技术选型，控制整体成本
- **专注核心**：将开发精力集中在 RAG 和知识图谱等核心技术上
- **灵活扩展**：支持多种数据源（GitHub API / MCP 服务）和模型方案，便于未来扩展
- **Pipeline 模式**：文件驱动，支持断点调试和人工介入，提高开发效率

**当前进展**：
- ✅ 已完成数据采集 Pipeline（Fetch/Clean/Ingest）
- ✅ 已成功提取 OpenAI Cookbook（256 个文档）
- ✅ 已配置混合存储架构（PostgreSQL + Neo4j）
- 🔄 正在进行知识图谱构建和双层检索实现

